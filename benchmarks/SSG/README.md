# SSG Benchmark

# Env

```bash
conda create -n ssg python=3.8.19
 
pip install https://download.pytorch.org/whl/cu117/torch-1.13.1%2Bcu117-cp38-cp38-linux_x86_64.whl

pip install -r requirements.txt
```

Download [Official trained BARTScore (on ParaBank2)](https://drive.google.com/file/d/1_7JfF7KOInb7ZrxKHIigTMR4ChVET01m/view?usp=sharing) and save in `score/bart_score.pth` .

# Prepare SSG dataset

**There we use `get_ssg_dataset/sample_raw_dataset` to simulate the whole dataset `dataset_v1.0_noaudio`.**

```bash
# 1. get the related sentences
python get_ssg_dataset/0get_related.py
# 2. get the ssg dataset
python get_ssg_dataset/1get_dataset.py
```

# Fine-tune LLaMA-2

1. Please download [https://huggingface.co/meta-llama/Llama-2-7b-hf/tree/main](https://huggingface.co/meta-llama/Llama-2-7b-hf/tree/main) and put it at `ssg_llama2/statics/debug.llama2_7b`.
2. Fine-tune LLaMA-2.

```bash
# slide2speech, stored in `l2p`
python train.py --task l2p --exp l2p
# slide2speech, only CHI Ubi subset, stored in `l2p,subset`
python train.py --task l2p --subset --exp l2p,subset
# slide2speech, only CHI Ubi subset, add paper, stored in `l2p,paper`
python train.py --task l2p --paper --exp l2p,paper

# speech2slide, stored in `p2l`
python train.py --task p2l --exp p2l
# speech2slide, only CHI Ubi subset, stored in `p2l,subset`
python train.py --task p2l --subset --exp p2l,subset
# speech2slide, only CHI Ubi subset, add paper, stored in `p2l,paper`
python train.py --task p2l --paper --exp p2l,paper
```

1. Generate on the test set.

```bash
# python generate.py --exp {the result directory}
python generate.py --exp l2p
```

# Run GPT-4(V)

1. Run the following commands.

```bash
# GPT-4 for slide2speech, speech2slide
python ssg_gpt4/gpt4_generate.py gpt-4

# GPT-4V for slide2speech
python ssg_gpt4/gpt4_generate.py gpt-4V
```

1. The results will be stored in `ssg_gpt4/gpt-4`  and `ssg_gpt4/gpt-4V` . **Please merge the results generated by individual api's between scoring, as the results are in the form of each videoâ€™s result.**

# Scoring

The input file should be like:

```json
[
  {
    "question": "",
    "answer": "",
    "generated_answer": ""
  },
  ...
 ]
```

or

```json
[
  {
    "speech_text": "",
    "ocr_text": "",
    "image_name": "",
    "generated_speech_text": ""
  },
  ...
 ]
```

or

```json
[
  {
    "speech_text": "",
    "ocr_text": "",
    "image_name": "",
    "generated_ocr_text": ""
  },
  ...
 ]
```

1. ROUGE score

```json
python score/score_rouge.py {input_file}
```

1. BERTScore & BARTScore

```json
python score/score_brt.py {input_file}
```

The scoring results will be stored near the input file. 

1. You can refer to `score/sample`  to check the correctness of scoring.